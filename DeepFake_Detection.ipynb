{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adi-204/DeepFake/blob/main/DeepFake_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9Lt5g6h6NnL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
        "from torchvision import transforms, models\n",
        "from torch import nn, optim\n",
        "from PIL import Image\n",
        "import random\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "TFBdrxAe6QzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Path to the folder containing video files in your Google Drive\n",
        "video_folder_path = '/content/drive/MyDrive/Celeb-DF-v2.zip (Unzipped Files)/Celeb-synthesis'\n",
        "\n",
        "# Output folder for extracted frames\n",
        "output_frames_path = '/content/drive/MyDrive/Single_Extracted/Celeb-synthesis'\n",
        "os.makedirs(output_frames_path, exist_ok=True)\n",
        "\n",
        "# Number of frames to extract from each video\n",
        "num_frames_to_extract = 1\n",
        "\n",
        "# Iterate through all video files in the folder\n",
        "for video_file in os.listdir(video_folder_path):\n",
        "    if video_file.endswith(('.mp4', '.avi', '.mov')):  # Add other video formats if needed\n",
        "        video_path = os.path.join(video_folder_path, video_file)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        frame_count = 0\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        step = max(1, total_frames // num_frames_to_extract)\n",
        "\n",
        "        success, frame = cap.read()\n",
        "        while success and frame_count < num_frames_to_extract:\n",
        "            # Save frame as an image file\n",
        "            frame_filename = f\"{os.path.splitext(video_file)[0]}_frame{frame_count}.jpg\"\n",
        "            cv2.imwrite(os.path.join(output_frames_path, frame_filename), frame)\n",
        "\n",
        "            # Move to the next frame based on the step\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, step * (frame_count + 1))\n",
        "            frame_count += 1\n",
        "            success, frame = cap.read()\n",
        "\n",
        "        cap.release()\n",
        "        print(f\"Extracted {frame_count} frames from {video_file}\")\n",
        "\n",
        "print(\"Frame extraction completed.\")"
      ],
      "metadata": {
        "id": "C9Vp9pIF6TRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dlib opencv-python imutils"
      ],
      "metadata": {
        "id": "sm1r8hTWZljV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget  # Install the wget module\n",
        "import wget\n",
        "import os\n",
        "\n",
        "# Download if file doesn't exist\n",
        "if not os.path.exists('shape_predictor_68_face_landmarks.dat'):\n",
        "    print(\"Downloading facial landmark predictor...\")\n",
        "    wget.download('http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n",
        "    !bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\n",
        "else:\n",
        "    print(\"Facial landmark predictor file already exists\")"
      ],
      "metadata": {
        "id": "oQfzuqz2aEsx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import face_utils\n",
        "\n",
        "def detect_and_crop_features(image_path):\n",
        "    # Initialize dlib's face detector and facial landmarks predictor\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "    # Read the image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(\"Unable to load image\")\n",
        "\n",
        "    # Convert to RGB for display\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Initialize dictionary to store crops\n",
        "    crops = {}\n",
        "\n",
        "    # Detect faces\n",
        "    faces = detector(img_rgb)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        print(\"No faces detected in the image\")\n",
        "        return None\n",
        "\n",
        "    # Process first face found\n",
        "    face = faces[0]\n",
        "\n",
        "    # Get facial landmarks\n",
        "    landmarks = predictor(img_rgb, face)\n",
        "    landmarks = face_utils.shape_to_np(landmarks)\n",
        "\n",
        "    # Define facial feature indices\n",
        "    FACIAL_LANDMARKS_IDXS = {\n",
        "        \"left_eye\": (36, 42),\n",
        "        \"right_eye\": (42, 48),\n",
        "        \"nose\": (27, 36),\n",
        "        \"face\": (0, 68)\n",
        "    }\n",
        "\n",
        "    # Function to get bounding box with padding\n",
        "    def get_bbox_with_padding(points, padding_percent=10):\n",
        "        x = points[:, 0]\n",
        "        y = points[:, 1]\n",
        "\n",
        "        # Calculate bounding box\n",
        "        x1, y1 = np.min(x), np.min(y)\n",
        "        x2, y2 = np.max(x), np.max(y)\n",
        "\n",
        "        # Calculate padding\n",
        "        width = x2 - x1\n",
        "        height = y2 - y1\n",
        "        padding_x = int(width * padding_percent / 100)\n",
        "        padding_y = int(height * padding_percent / 100)\n",
        "\n",
        "        # Apply padding\n",
        "        x1 = max(0, x1 - padding_x)\n",
        "        y1 = max(0, y1 - padding_y)\n",
        "        x2 = min(img_rgb.shape[1], x2 + padding_x)\n",
        "        y2 = min(img_rgb.shape[0], y2 + padding_y)\n",
        "\n",
        "        return (x1, y1, x2, y2)\n",
        "\n",
        "    # Crop face\n",
        "    face_box = (face.left(), face.top(), face.right(), face.bottom())\n",
        "    face_img = Image.fromarray(img_rgb).crop(face_box)\n",
        "    crops['face'] = face_img\n",
        "\n",
        "    # Get both eyes in one crop\n",
        "    left_eye_points = landmarks[36:42]\n",
        "    right_eye_points = landmarks[42:48]\n",
        "    eyes_points = np.vstack((left_eye_points, right_eye_points))\n",
        "    eyes_bbox = get_bbox_with_padding(eyes_points, padding_percent=30)\n",
        "    crops['eyes'] = Image.fromarray(img_rgb).crop(eyes_bbox)\n",
        "\n",
        "    # Get nose\n",
        "    nose_points = landmarks[27:36]\n",
        "    nose_bbox = get_bbox_with_padding(nose_points, padding_percent=20)\n",
        "    crops['nose'] = Image.fromarray(img_rgb).crop(nose_bbox)\n",
        "\n",
        "    return crops\n",
        "\n",
        "def display_crops(crops):\n",
        "    if crops is None:\n",
        "        print(\"No crops to display\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for idx, (feature, crop_img) in enumerate(crops.items(), 1):\n",
        "        plt.subplot(1, 3, idx)\n",
        "        plt.imshow(crop_img)\n",
        "        plt.title(f\"Cropped {feature}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "try:\n",
        "    # Replace with your image path\n",
        "    image_path = '/content/drive/MyDrive/Colab Notebooks/Single_Extracted/Celeb-real/00066_frame0.jpg'\n",
        "\n",
        "    # Detect and crop facial features\n",
        "    crops = detect_and_crop_features(image_path)\n",
        "\n",
        "    # Display results\n",
        "    display_crops(crops)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "dmVg4QySGyk6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepFakeDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Validate data directory\n",
        "        if not os.path.isdir(data_dir):\n",
        "            raise ValueError(f\"Invalid directory: {data_dir}\")\n",
        "\n",
        "        # Collect image paths and labels\n",
        "        for filename in os.listdir(data_dir):\n",
        "            if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "                full_path = os.path.join(data_dir, filename)\n",
        "                # Validate image file\n",
        "                try:\n",
        "                    with Image.open(full_path) as img:\n",
        "                        img.verify()\n",
        "                    self.image_paths.append(full_path)\n",
        "                    label = 1 if \"real\" in filename.lower() else 0\n",
        "                    self.labels.append(label)\n",
        "                except Exception as e:\n",
        "                    print(f\"Skipping invalid image {full_path}: {str(e)}\")\n",
        "\n",
        "        if not self.image_paths:\n",
        "            raise ValueError(f\"No valid images found in {data_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "            # Detect and crop features\n",
        "            crops = detect_and_crop_features(img_path)\n",
        "\n",
        "            if crops is None:\n",
        "                # Fallback to original image if feature detection fails\n",
        "                img = Image.open(img_path)\n",
        "\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "\n",
        "                return img, img, img, self.labels[idx]\n",
        "\n",
        "            # Apply transforms if specified\n",
        "            if self.transform:\n",
        "                face_img = self.transform(crops.get('face'))\n",
        "                eye_img = self.transform(crops.get('eyes'))\n",
        "                nose_img = self.transform(crops.get('nose'))\n",
        "            else:\n",
        "                face_img = crops.get('face')\n",
        "                eye_img = crops.get('eyes')\n",
        "                nose_img = crops.get('nose')\n",
        "\n",
        "            return eye_img, nose_img, face_img, self.labels[idx]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in __getitem__: {str(e)}\")\n",
        "            # Return black placeholder images\n",
        "            placeholder = torch.zeros(3, 224, 224)\n",
        "            return placeholder, placeholder, placeholder, self.labels[idx]\n"
      ],
      "metadata": {
        "id": "hSXk621p6Vg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oversample_dataset(real_dataset, fake_dataset):\n",
        "    # Validate inputs\n",
        "    if not isinstance(real_dataset, DeepFakeDataset) or not isinstance(fake_dataset, DeepFakeDataset):\n",
        "        raise ValueError(\"Inputs must be DeepFakeDataset instances\")\n",
        "\n",
        "    real_count = len(real_dataset)\n",
        "    fake_count = len(fake_dataset)\n",
        "\n",
        "    # Oversample real frames to match fake frames\n",
        "    oversampled_real_frames = []\n",
        "    while len(oversampled_real_frames) < fake_count:\n",
        "        oversampled_real_frames.extend(real_dataset.image_paths)\n",
        "\n",
        "    # Shuffle and trim to exact count\n",
        "    random.shuffle(oversampled_real_frames)\n",
        "    oversampled_real_frames = oversampled_real_frames[:fake_count]\n",
        "\n",
        "    # Combine frames and labels\n",
        "    combined_frames = oversampled_real_frames + fake_dataset.image_paths\n",
        "    combined_labels = [1] * len(oversampled_real_frames) + [0] * len(fake_dataset.image_paths)\n",
        "\n",
        "    return combined_frames, combined_labels\n",
        "\n",
        "# Data transforms (e.g., resizing and normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets for real and fake frames\n",
        "real_dataset = DeepFakeDataset(data_dir=\"/content/drive/MyDrive/Colab Notebooks/Single_Extracted/Celeb-real\", transform=transform)\n",
        "fake_dataset = DeepFakeDataset(data_dir=\"/content/drive/MyDrive/Colab Notebooks/Single_Extracted/Celeb-synthesis\", transform=transform)\n",
        "\n",
        "\n",
        "# Perform oversampling on the real dataset to match the size of the fake dataset\n",
        "combined_frames, combined_labels = oversample_dataset(real_dataset, fake_dataset)\n",
        "\n",
        "print(\"\\nDataset Structure:\")\n",
        "print(f\"Total combined frames: {len(combined_frames)}\")\n",
        "print(f\"Real frames (label 1): {combined_labels.count(1)}\")\n",
        "print(f\"Fake frames (label 0): {combined_labels.count(0)}\")\n",
        "print(\"First 5 Combined Frames:\")\n",
        "for i in range(min(5, len(combined_frames))):\n",
        "    print(f\"{i+1}. {os.path.basename(combined_frames[i])} - Label: {combined_labels[i]}\")"
      ],
      "metadata": {
        "id": "GhQAEctE6YMc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedDataset(Dataset):\n",
        "    def __init__(self, frames, labels, transform=None):\n",
        "        self.frames = frames\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      try:\n",
        "        img_path = self.frames[idx]\n",
        "        image = Image.open(img_path)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        crops = detect_and_crop_features(img_path)\n",
        "\n",
        "        if crops is None:\n",
        "            # Handle the case where crops is None, e.g., skip this sample or raise an error\n",
        "          print(f\"Warning: crops is None for index {idx}. Skipping this sample.\")\n",
        "          return None # Or raise an exception, depending on your desired behavior\n",
        "\n",
        "\n",
        "        if crops.get('face') is None or crops.get('eyes') is None or crops.get('nose') is None:\n",
        "            print(f\"Warning: Skipping index {idx} due to missing data.\")\n",
        "            return None  # Skip this sample (can result in imbalanced data)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            face_img = self.transform(crops.get('face'))\n",
        "            eye_img = self.transform(crops.get('eyes'))\n",
        "            nose_img = self.transform(crops.get('nose'))\n",
        "\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return eye_img, nose_img, face_img, label\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"Error at index {idx}: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# Create a new combined dataset with oversampling applied\n",
        "combined_dataset = CombinedDataset(combined_frames, combined_labels, transform=transform)\n",
        "\n",
        "# Dataset splitting: 70% training, 15% validation, 15% testing\n",
        "total_length = len(combined_dataset)\n",
        "train_len = int(0.7 * total_length)\n",
        "val_len = int(0.15 * total_length)\n",
        "test_len = total_length - train_len - val_len\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(combined_dataset, [train_len, val_len, test_len])\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "RSfKxpVg6bJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "num_images_to_display = 5\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    eye_img, nose_img, face_img, label = combined_dataset[i]\n",
        "\n",
        "    # Function to denormalize and display an image\n",
        "    def display_image(img, title):\n",
        "        img_np = img.cpu().numpy().transpose(1, 2, 0)\n",
        "        img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "        plt.imshow(img_np)\n",
        "        plt.title(title)\n",
        "        plt.axis('off')  # Turn off axis labels and ticks\n",
        "        plt.show()\n",
        "\n",
        "    # Display eye_img\n",
        "    display_image(eye_img, f\"Eye - Label: {label}\")\n",
        "\n",
        "    # Display nose_img\n",
        "    display_image(nose_img, f\"Nose - Label: {label}\")\n",
        "\n",
        "    # Display face_img\n",
        "    display_image(face_img, f\"Face - Label: {label}\")"
      ],
      "metadata": {
        "id": "RHVkyYHR4S3Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "# Backbone model with branches for eye, nose, and face detection\n",
        "class BackboneWithBranches(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BackboneWithBranches, self).__init__()\n",
        "        self.shared_backbone = nn.Sequential(*(list(resnet.children())[:-2]))\n",
        "\n",
        "        # Eye Detection Branch\n",
        "        self.eye_conv = nn.Sequential(\n",
        "            nn.Conv2d(2048, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.eye_fc = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.5)\n",
        "        )\n",
        "        self.eye_classifier = nn.Linear(256, 2)\n",
        "\n",
        "        # Nose Detection Branch\n",
        "        self.nose_conv = nn.Sequential(\n",
        "            nn.Conv2d(2048, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.nose_fc = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.5)\n",
        "        )\n",
        "        self.nose_classifier = nn.Linear(256, 2)\n",
        "\n",
        "        # Face Detection Branch\n",
        "        self.face_conv = nn.Sequential(\n",
        "            nn.Conv2d(2048, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.face_fc = nn.Sequential(\n",
        "                nn.Linear(512 * 3 * 3, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.5)\n",
        "        )\n",
        "        self.face_classifier = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, eye_img, nose_img, face_img):\n",
        "        # Feature extraction for eye\n",
        "        eye_features = self.shared_backbone(eye_img)\n",
        "        eye_features = self.eye_conv(eye_features)\n",
        "        eye_features = eye_features.view(eye_features.size(0), -1)  # Flatten\n",
        "        eye_output = self.eye_classifier(self.eye_fc(eye_features))\n",
        "\n",
        "        # Feature extraction for nose\n",
        "        nose_features = self.shared_backbone(nose_img)\n",
        "        nose_features = self.nose_conv(nose_features)\n",
        "        nose_features = nose_features.view(nose_features.size(0), -1)\n",
        "        nose_output = self.nose_classifier(self.nose_fc(nose_features))\n",
        "\n",
        "        # Feature extraction for face\n",
        "        face_features = self.shared_backbone(face_img)\n",
        "        face_features = self.face_conv(face_features)\n",
        "        face_features = face_features.view(face_features.size(0), -1)\n",
        "        face_output = self.face_classifier(self.face_fc(face_features))\n",
        "\n",
        "        return eye_output, nose_output, face_output\n"
      ],
      "metadata": {
        "id": "hRocw7yX6dk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def majority_voting(eye_output, nose_output, face_output):\n",
        "    outputs = torch.stack([eye_output, nose_output, face_output], dim=0)\n",
        "    final_output = outputs.mean(dim=0)\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "EHwtrzzlIP58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade torch ultralytics"
      ],
      "metadata": {
        "id": "hr-PZVS9qnJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=10, val_loader=None, save_path=\"model_checkpoint.pth\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (eye_img, nose_img, face_img, labels) in enumerate(train_loader):\n",
        "            if eye_img is None or nose_img is None or face_img is None or labels is None:\n",
        "                print(f\"Warning: Skipping batch {batch_idx} due to missing data.\")\n",
        "                continue  # Skip this batch and move to the next\n",
        "\n",
        "            eye_img, nose_img, face_img, labels = eye_img.to(device), nose_img.to(device), face_img.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            eye_output, nose_output, face_output = model(eye_img, nose_img, face_img)\n",
        "            final_output = majority_voting(eye_output, nose_output, face_output)\n",
        "            loss = criterion(final_output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(final_output.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        running_loss /= len(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "        # Validation loop (if validation data is provided)\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for val_batch in val_loader:\n",
        "                    eye_img_val, nose_img_val, face_img_val, labels_val = val_batch\n",
        "                    eye_img_val, nose_img_val, face_img_val, labels_val = (\n",
        "                        eye_img_val.to(device), nose_img_val.to(device), face_img_val.to(device), labels_val.to(device)\n",
        "                    )\n",
        "\n",
        "                    eye_output, nose_output, face_output = model(eye_img_val, nose_img_val, face_img_val)\n",
        "                    final_output_val = majority_voting(eye_output, nose_output, face_output)\n",
        "                    loss_val = criterion(final_output_val, labels_val)\n",
        "                    val_loss += loss_val.item()\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save model checkpoint after each epoch\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': running_loss,\n",
        "        }\n",
        "        torch.save(checkpoint, save_path)\n",
        "        print(f\"Model saved at epoch {epoch+1} to {save_path}\")\n",
        "\n",
        "# Model, criterion, and optimizer setup\n",
        "model = BackboneWithBranches()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
        "num_epochs = 3\n",
        "\n",
        "# Start training with train_loader and val_loader\n",
        "train(model, train_loader, criterion, optimizer, num_epochs, val_loader=val_loader, save_path=\"/content/drive/MyDrive/Colab Notebooks/deepfake_detector_check.pth\")"
      ],
      "metadata": {
        "id": "qhpc8LLw6gMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_checkpoint(model, optimizer, checkpoint_path):\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    val_loss = checkpoint['loss']\n",
        "\n",
        "    print(f\"Checkpoint loaded. Resuming from epoch {epoch} with validation loss {val_loss:.4f}\")\n",
        "    print(f\"Model state dict keys: {model.state_dict().keys()}\")\n",
        "\n",
        "    return model, optimizer, epoch, val_loss\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/deepfake_detector_check.pth\"\n",
        "model, optimizer, start_epoch, best_val_loss = load_model_checkpoint(model, optimizer, checkpoint_path)"
      ],
      "metadata": {
        "id": "ul0v0M3r6jUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_model_with_timing(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_time = 0.0\n",
        "    total_images = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            eye_img, nose_img, face_img, labels = batch\n",
        "            batch_size = labels.size(0)\n",
        "            total_images += batch_size\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                eye_img, nose_img, face_img, labels = (\n",
        "                    eye_img.cuda(), nose_img.cuda(), face_img.cuda(), labels.cuda()\n",
        "                )\n",
        "\n",
        "            # Measure the time taken for each batch prediction\n",
        "            start_time = time.time()\n",
        "            eye_output, nose_output, face_output = model(eye_img, nose_img, face_img)\n",
        "            final_output = majority_voting(eye_output, nose_output, face_output)\n",
        "            end_time = time.time()\n",
        "\n",
        "            # Update total time\n",
        "            total_time += (end_time - start_time)\n",
        "\n",
        "            _, preds = torch.max(final_output, 1)\n",
        "            probs = torch.softmax(final_output, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate average time per batch and per image\n",
        "    avg_time_per_batch = total_time / len(data_loader)\n",
        "    avg_time_per_image = total_time / total_images\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels), np.array(all_probs), total_time, avg_time_per_batch, avg_time_per_image\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Actual Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(y_true, y_scores):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_precision_recall_curve(y_true, y_scores):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='b', lw=2)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.show()\n",
        "\n",
        "# Get predictions, ground truth, and timing information\n",
        "preds, labels, probs, total_time, avg_time_per_batch, avg_time_per_image = evaluate_model_with_timing(model, test_loader)\n",
        "\n",
        "# Accuracy, Precision, Recall, F1-Score\n",
        "accuracy = accuracy_score(labels, preds)\n",
        "precision = precision_score(labels, preds, average='weighted')\n",
        "recall = recall_score(labels, preds, average='weighted')\n",
        "f1 = f1_score(labels, preds, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Print timing information\n",
        "print(f\"Total time taken for predictions: {total_time:.4f} seconds\")\n",
        "print(f\"Average time per batch: {avg_time_per_batch:.4f} seconds\")\n",
        "print(f\"Average time per image: {avg_time_per_image:.6f} seconds\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plot_confusion_matrix(cm, classes=['Fake', 'Real'])\n",
        "\n",
        "# ROC and Precision-Recall curves (for binary classification)\n",
        "if len(np.unique(labels)) == 2:  # Check if it's binary classification\n",
        "    y_scores = probs[:, 1]  # Get probabilities for class 1\n",
        "    plot_roc_curve(labels, y_scores)\n",
        "    plot_precision_recall_curve(labels, y_scores)\n",
        "else:\n",
        "    print(\"ROC and Precision-Recall curves are only plotted for binary classification.\")\n"
      ],
      "metadata": {
        "id": "Jb754q8B6oZy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}